{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4265de10",
   "metadata": {},
   "source": [
    "# functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88e66fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import time\n",
    "import json\n",
    "import copy\n",
    "from itertools import product\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier, RidgeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier,AdaBoostClassifier, BaggingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import RadiusNeighborsClassifier, KNeighborsClassifier\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.utils import compute_sample_weight\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_curve,make_scorer,average_precision_score, matthews_corrcoef,precision_recall_curve,classification_report \\\n",
    ",accuracy_score, precision_score, recall_score, f1_score, balanced_accuracy_score, roc_auc_score,confusion_matrix, auc,brier_score_loss,\\\n",
    "fowlkes_mallows_score,cohen_kappa_score,jaccard_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db7ef32-33cb-4ec5-a803-455ad1ea0d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumpyArrayEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return json.JSONEncoder.default(self, obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d26855-21e5-4ccd-bc40-ae0390bf814d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_json(model_lite,model,model_name):\n",
    "    json_array = json.dumps(model_lite, cls=NumpyArrayEncoder,indent=2)\n",
    "    with open(f\"{model_name}_clf_lite.json\", 'w') as json_file:\n",
    "        json_file.write(json_array)\n",
    "    #json_array = json.dumps(model, cls=NumpyArrayEncoder,indent=2)\n",
    "    #with open(f\"{model_name}_clf.json\", 'w') as json_file:\n",
    "    #    json_file.write(json_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "117ed9bd-6e1c-45b9-845f-7e53935e8fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoring_metrics(y_test, y_pred, y_pred_prob):\n",
    "    scores = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'balanced_accuracy': balanced_accuracy_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred,zero_division=1),\n",
    "        'average_precision': average_precision_score(y_test, y_pred),\n",
    "        'recall': recall_score(y_test, y_pred),\n",
    "        'f1': f1_score(y_test, y_pred),\n",
    "        'jaccard': jaccard_score(y_test, y_pred),\n",
    "        'fowlkes_mallows': fowlkes_mallows_score(y_test, y_pred),\n",
    "        'cohen_kappa': cohen_kappa_score(y_test, y_pred),\n",
    "        'matthews_corrcoef': matthews_corrcoef(y_test, y_pred),\n",
    "        'pr_auc': average_precision_score(y_test, y_pred_prob), # No zero_division for average_precision_score\n",
    "        'roc_auc': roc_auc_score(y_test, y_pred_prob),\n",
    "    }\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "663d374b-f676-4fc5-a165-c278a4168b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(model, param_grid, X_outter, y_outter,sgd = False,class_weight = False):\n",
    "    best_score = None\n",
    "    best_params = None\n",
    "    best_model = None\n",
    "    param_comb_arr = []\n",
    "    param_comb_arr_lite = []\n",
    "    # Generate all possible combinations of hyperparameters\n",
    "    all_params = list(product(*param_grid.values()))\n",
    "\n",
    "    for params in all_params:\n",
    "        # Create a dictionary of hyperparameter values\n",
    "        param_dict = {param_name: param_value for param_name, param_value in zip(param_grid.keys(), params)}\n",
    "        param_comb = {}\n",
    "        param_comb_lite = {}\n",
    "        param_comb['params'] = param_dict\n",
    "        param_comb_lite['params'] = param_dict\n",
    "        \n",
    "        # Set the model hyperparameters\n",
    "        model.set_params(**param_dict)\n",
    "        inner_fold_arr = []\n",
    "        inner_fold_arr_lite = []\n",
    "        scores = []\n",
    "        for fold_num, (inner_train_index, inner_test_index) in enumerate(inner_cv.split(X_outter, y_outter)):\n",
    "            X_inner_train, X_inner_test = X_outter[inner_train_index], X_outter[inner_test_index]\n",
    "            y_inner_train, y_inner_test = y_outter[inner_train_index], y_outter[inner_test_index]\n",
    "            \n",
    "            inner_fold = {}\n",
    "            inner_fold_lite = {}\n",
    "            inner_fold['fold_num'] = fold_num+1\n",
    "            inner_fold_lite['fold_num'] = fold_num+1\n",
    "            test_row = {}\n",
    "            train_row = {}\n",
    "            \n",
    "            if(class_weight):\n",
    "                unique_classes = np.unique(y_inner_train)\n",
    "                class_counts = np.bincount(y_inner_train)\n",
    "                class_frequencies = {class_label: count / len(y_inner_train) for class_label, count in zip(unique_classes, class_counts)}\n",
    "                balanced_sample_weights = np.array([1.0 / class_frequencies[class_label] for class_label in y_inner_train])\n",
    "                start_time = time.time()\n",
    "                model.fit(X_inner_train, y_inner_train,sample_weight = balanced_sample_weights)\n",
    "                end_time = time.time()\n",
    "                train_row['fit_time']=end_time - start_time\n",
    "                \n",
    "            else:\n",
    "                start_time = time.time()\n",
    "                model.fit(X_inner_train, y_inner_train)\n",
    "                end_time = time.time()\n",
    "                train_row['fit_time']=end_time - start_time\n",
    "            \n",
    "            start_time = time.time()\n",
    "            y_pred = model.predict(X_inner_train)\n",
    "            end_time = time.time()\n",
    "            train_row['pred_time']=end_time - start_time\n",
    "            \n",
    "            if(sgd):\n",
    "                start_time = time.time()\n",
    "                y_pred_prob = cross_val_predict(model, X_inner_train, y_inner_train, cv=3,method=\"decision_function\")\n",
    "                end_time = time.time()\n",
    "                \n",
    "            else:\n",
    "                start_time = time.time()\n",
    "                y_pred_prob = model.predict_proba(X_inner_train)[:,1]\n",
    "                end_time = time.time()\n",
    "            train_row['pred_proba_time']=end_time - start_time\n",
    "\n",
    "            train_row.update(scoring_metrics(y_inner_train,y_pred, y_pred_prob))\n",
    "            \n",
    "            inner_fold_lite['train'] =  copy.deepcopy(train_row)\n",
    "            train_row['y'] = y_inner_train\n",
    "            train_row['y_pred'] = y_pred\n",
    "            train_row['y_pred_prob'] = y_pred_prob\n",
    "            train_row['indices'] = inner_train_index\n",
    "\n",
    "            inner_fold['train']=train_row\n",
    "\n",
    "            start_time = time.time()\n",
    "            y_pred = model.predict(X_inner_test)\n",
    "            end_time = time.time()\n",
    "            test_row['pred_time']=end_time - start_time\n",
    "        \n",
    "            if(sgd):\n",
    "                start_time = time.time()\n",
    "                y_pred_prob = cross_val_predict(model, X_inner_test, y_inner_test, cv=3,method=\"decision_function\")\n",
    "                end_time = time.time()\n",
    "                \n",
    "            else:\n",
    "                start_time = time.time()\n",
    "                y_pred_prob = model.predict_proba(X_inner_test)[:,1]\n",
    "                end_time = time.time()\n",
    "            test_row['pred_proba_time']=end_time - start_time\n",
    "            \n",
    "            \n",
    "            test_metrics = scoring_metrics(y_inner_test,y_pred, y_pred_prob)\n",
    "            test_row.update(test_metrics)\n",
    "            scores.append(test_metrics)\n",
    "            \n",
    "            inner_fold_lite['test']=copy.deepcopy(test_row)\n",
    "            \n",
    "            test_row['y'] = y_inner_test\n",
    "            test_row['y_pred'] = y_pred\n",
    "            test_row['y_pred_prob'] = y_pred_prob\n",
    "            test_row['indices'] = inner_test_index\n",
    "\n",
    "            inner_fold['test']=test_row\n",
    "            inner_fold_arr.append(inner_fold)\n",
    "            inner_fold_arr_lite.append(inner_fold_lite)\n",
    "            \n",
    "        total_f1 = 0.0\n",
    "        for data in scores:\n",
    "            total_f1 += data['f1']\n",
    "        score_f1 = total_f1 / 3\n",
    "\n",
    "        if best_score is None or score_f1 > best_score:\n",
    "            best_score = score_f1\n",
    "            best_params = param_dict\n",
    "            best_model = copy.deepcopy(model)\n",
    "        param_comb['inner_fold'] = inner_fold_arr\n",
    "        param_comb_lite['inner_fold'] = inner_fold_arr_lite\n",
    "        param_comb_arr.append(param_comb)\n",
    "        param_comb_arr_lite.append(param_comb_lite)\n",
    "\n",
    "\n",
    "    return best_model,best_params,param_comb_arr,param_comb_arr_lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6917811c-0d18-4380-adf7-744cfe42c6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def outer_metric(model, params, X_out_train, y_out_train,idx_train,X_out_test,y_out_test,idx_test,outer_fold_num,sgd = False, class_weight = False):\n",
    "  outer_fold = {}\n",
    "  outer_fold_lite = {}\n",
    "  test_row = {}\n",
    "  train_row = {}\n",
    "  \n",
    "  outer_fold['fold_num']= int(outer_fold_num)\n",
    "  outer_fold['best_params'] = params\n",
    "  outer_fold_lite['fold_num']= int(outer_fold_num)\n",
    "  outer_fold_lite['best_params'] = params\n",
    "  \n",
    "  if(class_weight):\n",
    "    unique_classes = np.unique(y_out_train)\n",
    "    class_counts = np.bincount(y_out_train)\n",
    "    class_frequencies = {class_label: count / len(y_out_train) for class_label, count in zip(unique_classes, class_counts)}\n",
    "    balanced_sample_weights = np.array([1.0 / class_frequencies[class_label] for class_label in y_out_train])\n",
    "    start_time = time.time()\n",
    "    model.fit(X_out_train, y_out_train,sample_weight = balanced_sample_weights)\n",
    "    end_time = time.time()\n",
    "    train_row['fit_time']=end_time - start_time\n",
    "    \n",
    "  else:\n",
    "    start_time = time.time()\n",
    "    model.fit(X_out_train, y_out_train)\n",
    "    end_time = time.time()\n",
    "    train_row['fit_time']=end_time - start_time\n",
    "\n",
    "  start_time = time.time()\n",
    "  y_pred = model.predict(X_out_train)\n",
    "  end_time = time.time()\n",
    "  train_row['pred_time']=end_time - start_time\n",
    "  if(sgd):\n",
    "    start_time = time.time()\n",
    "    y_pred_prob = cross_val_predict(model, X_out_train, y_out_train, cv=3,method=\"decision_function\")\n",
    "    end_time = time.time()\n",
    "    \n",
    "  else:\n",
    "    start_time = time.time()\n",
    "    y_pred_prob = model.predict_proba(X_out_train)[:,1]\n",
    "    end_time = time.time()\n",
    "  train_row['pred_proba_time']=end_time - start_time\n",
    "\n",
    "  train_row.update(scoring_metrics(y_out_train,y_pred, y_pred_prob))\n",
    "\n",
    "  outer_fold_lite['train']=copy.deepcopy(train_row)\n",
    "  \n",
    "  train_row['y'] = y_out_train\n",
    "  train_row['y_pred'] = y_pred\n",
    "  train_row['y_pred_prob'] = y_pred_prob\n",
    "  train_row['indices'] = idx_train\n",
    "\n",
    "  outer_fold['train']=train_row\n",
    "\n",
    "  start_time = time.time()\n",
    "  y_pred = model.predict(X_out_test)\n",
    "  end_time = time.time()\n",
    "  test_row['pred_time']=end_time - start_time\n",
    "      \n",
    "  if(sgd):\n",
    "    start_time = time.time()\n",
    "    y_pred_prob = cross_val_predict(model, X_out_test, y_out_test, cv=3,method=\"decision_function\")\n",
    "    end_time = time.time()\n",
    "    \n",
    "  else:\n",
    "    start_time = time.time()\n",
    "    y_pred_prob = model.predict_proba(X_out_test)[:,1]\n",
    "    end_time = time.time()\n",
    "  test_row['pred_proba_time']=end_time - start_time\n",
    "\n",
    "  test_row.update(scoring_metrics(y_out_test,y_pred, y_pred_prob))\n",
    "\n",
    "  outer_fold_lite['test']=copy.deepcopy(test_row)\n",
    "  \n",
    "  test_row['y'] = y_out_test\n",
    "  test_row['y_pred'] = y_pred\n",
    "  test_row['y_pred_prob'] = y_pred_prob\n",
    "  test_row['indices'] = idx_test\n",
    "\n",
    "  outer_fold['test']=test_row\n",
    "  return outer_fold,outer_fold_lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e8805ca-66f6-495b-a5f2-a7326ee8d438",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_fold(model,fold_num, outer_train_index, outer_test_index, sgd = False,class_weight = False):\n",
    "    X_outer_train, X_outer_test = Xarr[outer_train_index], Xarr[outer_test_index]\n",
    "    y_outer_train, y_outer_test = yarr[outer_train_index], yarr[outer_test_index]\n",
    "\n",
    "    best_model, best_param, param_arr, param_arr_lite = grid_search(model, param_grid, X_outer_train, y_outer_train, sgd, class_weight)\n",
    "    outer_loop, outer_loop_lite = outer_metric(best_model, best_param, X_outer_train, y_outer_train, outer_train_index,\n",
    "                                                X_outer_test, y_outer_test, outer_test_index, fold_num + 1,sgd, class_weight)\n",
    "    outer_loop['param_comb'] = param_arr\n",
    "    outer_loop_lite['param_comb'] = param_arr_lite\n",
    "    print(f'Finished fold {fold_num + 1}')\n",
    "    return outer_loop, outer_loop_lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189f17a3-b671-47b5-81a3-49b7f85401a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "momental_dataset = \"6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72af540-55b6-4eee-a11c-151d32bd2245",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumpyArrayEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return json.JSONEncoder.default(self, obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa5b823-1847-45cf-b9d4-34f9765d1a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_json(model_lite,model,model_name):\n",
    "    json_array = json.dumps(model_lite, cls=NumpyArrayEncoder,indent=2)\n",
    "    with open(f\"result_dataset_{momental_dataset}/{model_name}_reg_lite.json\", 'w') as json_file:\n",
    "        json_file.write(json_array)\n",
    "    #json_array = json.dumps(model, cls=NumpyArrayEncoder,indent=2)\n",
    "    #with open(f\"result_dataset_{momental_dataset}/{model_name}_reg.json\", 'w') as json_file:\n",
    "    #    json_file.write(json_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5209cb0c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# load the data and prepare it for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc97d72d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>hypertension</th>\n",
       "      <th>heart_disease</th>\n",
       "      <th>ever_married</th>\n",
       "      <th>work_type</th>\n",
       "      <th>Residence_type</th>\n",
       "      <th>avg_glucose_level</th>\n",
       "      <th>bmi</th>\n",
       "      <th>smoking_status</th>\n",
       "      <th>stroke</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Male</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Private</td>\n",
       "      <td>Urban</td>\n",
       "      <td>228.69</td>\n",
       "      <td>36.6</td>\n",
       "      <td>formerly smoked</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Male</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Private</td>\n",
       "      <td>Rural</td>\n",
       "      <td>105.92</td>\n",
       "      <td>32.5</td>\n",
       "      <td>never smoked</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Female</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Private</td>\n",
       "      <td>Urban</td>\n",
       "      <td>171.23</td>\n",
       "      <td>34.4</td>\n",
       "      <td>smokes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Female</td>\n",
       "      <td>79.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Self-employed</td>\n",
       "      <td>Rural</td>\n",
       "      <td>174.12</td>\n",
       "      <td>24.0</td>\n",
       "      <td>never smoked</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Male</td>\n",
       "      <td>81.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Private</td>\n",
       "      <td>Urban</td>\n",
       "      <td>186.21</td>\n",
       "      <td>29.0</td>\n",
       "      <td>formerly smoked</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gender   age  hypertension  heart_disease ever_married      work_type  \\\n",
       "0    Male  67.0             0              1          Yes        Private   \n",
       "1    Male  80.0             0              1          Yes        Private   \n",
       "2  Female  49.0             0              0          Yes        Private   \n",
       "3  Female  79.0             1              0          Yes  Self-employed   \n",
       "4    Male  81.0             0              0          Yes        Private   \n",
       "\n",
       "  Residence_type  avg_glucose_level   bmi   smoking_status  stroke  \n",
       "0          Urban             228.69  36.6  formerly smoked       1  \n",
       "1          Rural             105.92  32.5     never smoked       1  \n",
       "2          Urban             171.23  34.4           smokes       1  \n",
       "3          Rural             174.12  24.0     never smoked       1  \n",
       "4          Urban             186.21  29.0  formerly smoked       1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(f'./dataset{momental_dataset}/procesed_dataset_{momental_dataset}.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac5e079a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(\"stroke\", axis=1)\n",
    "y = df[\"stroke\"]\n",
    "\n",
    "# Convert the features and labels to numpy arrays\n",
    "Xarr = np.array(X)\n",
    "yarr = np.array(y)\n",
    "outer_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "inner_cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04eb5005-54fe-4b30-b133-75b085bfcfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = []\n",
    "all_models_lite = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca351b1-826d-49b5-819a-7840ee8d7565",
   "metadata": {},
   "source": [
    "# Training the models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20389dd3-5ed2-4755-91fd-4f19badffdfc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## AdaBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d222491d-8c06-4baf-b7f7-8dc5b2b6e4a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'algorithm': 'SAMME.R',\n",
       " 'base_estimator': 'deprecated',\n",
       " 'estimator': None,\n",
       " 'learning_rate': 1.0,\n",
       " 'n_estimators': 50,\n",
       " 'random_state': 42}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada_clf = AdaBoostClassifier(random_state = 42)\n",
    "#ada_clf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a9342dd7-323a-4898-91af-0e5b8eee82f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.93566370010376\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200, 300, 400, 500, 700],  # Number of weak learners (base estimators).\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.5, 1.0],  # Shrinkage parameter to control learning rate. Smaller values reduce overfitting.\n",
    "    'estimator': [DecisionTreeClassifier(max_depth=1), DecisionTreeClassifier(max_depth=3), DecisionTreeClassifier(max_depth=7)],  # Base estimator. Simpler models can reduce overfitting.\n",
    "    'algorithm': ['SAMME', 'SAMME.R'],  # Algorithm for updating weights. SAMME.R is recommended for better convergence.\n",
    "}\n",
    "ada_model = {}\n",
    "ada_model['model']='AdaBoost_Classifier'\n",
    "outer_loop_arr = []\n",
    "ada_model_lite = {}\n",
    "ada_model_lite['model']='AdaBoost_Classifier'\n",
    "outer_loop_arr_lite = []\n",
    "\n",
    "starting_time = time.time()\n",
    "\n",
    "\n",
    "outer_loop_results = Parallel(n_jobs=-1)(\n",
    "    delayed(process_fold)(ada_clf, param_grid, fold_num, outer_train_index, outer_test_index,False,True)\n",
    "    for fold_num, (outer_train_index, outer_test_index) in enumerate(outer_cv.split(X, y))\n",
    ")\n",
    "\n",
    "# Extract the results for each fold\n",
    "outer_loop_arr = [result[0] for result in outer_loop_results]\n",
    "outer_loop_arr_lite = [result[1] for result in outer_loop_results]\n",
    "\n",
    "# Continue with appending the results to your all_models and all_models_lite lists\n",
    "ada_model['outer_loop'] = outer_loop_arr\n",
    "ada_model_lite['outer_loop'] = outer_loop_arr_lite\n",
    "all_models.append(ada_model)\n",
    "all_models_lite.append(ada_model_lite)\n",
    "to_json(ada_model_lite,ada_model,'ada')\n",
    "\n",
    "ending_time = time.time()\n",
    "print(ending_time - starting_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53540d32-5637-4eb8-a816-8c70d1523539",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Bagging Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a01105be-00c1-44f8-95e8-35934422364f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_estimator': 'deprecated',\n",
       " 'bootstrap': True,\n",
       " 'bootstrap_features': False,\n",
       " 'estimator': None,\n",
       " 'max_features': 1.0,\n",
       " 'max_samples': 1.0,\n",
       " 'n_estimators': 10,\n",
       " 'n_jobs': -1,\n",
       " 'oob_score': False,\n",
       " 'random_state': 42,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_reg = BaggingClassifier(n_jobs = -1, random_state = 42)\n",
    "#bag_reg.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a962ca5f-3df9-464c-b891-683fb01f3b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [10, 50, 100, 200, 300, 400, 500],  # Number of base estimators (bags). Larger values lead to stronger regularization.\n",
    "    'base_estimator': [None, RidgeClassifier(alpha=1.0), DecisionTreeClassifier()],  # Base estimator to use. \n",
    "    'max_samples': [0.7, 0.8, 0.9, 1.0],  # Fraction of samples used for fitting each bag. Larger values lead to stronger regularization.\n",
    "}\n",
    "bag_model = {}\n",
    "bag_model['model']='Bagging_Classifier'\n",
    "outer_loop_arr = []\n",
    "bag_model_lite = {}\n",
    "bag_model_lite['model']='Bagging_Classifier'\n",
    "outer_loop_arr_lite = []\n",
    "\n",
    "starting_time = time.time()\n",
    "\n",
    "\n",
    "outer_loop_results = Parallel(n_jobs=-1)(\n",
    "    delayed(process_fold)(bag_clf, param_grid, fold_num, outer_train_index, outer_test_index,False,True)\n",
    "    for fold_num, (outer_train_index, outer_test_index) in enumerate(outer_cv.split(X, y))\n",
    ")\n",
    "\n",
    "# Extract the results for each fold\n",
    "outer_loop_arr = [result[0] for result in outer_loop_results]\n",
    "outer_loop_arr_lite = [result[1] for result in outer_loop_results]\n",
    "\n",
    "# Continue with appending the results to your all_models and all_models_lite lists\n",
    "bag_model['outer_loop'] = outer_loop_arr\n",
    "bag_model_lite['outer_loop'] = outer_loop_arr_lite\n",
    "all_models.append(bag_model)\n",
    "all_models_lite.append(bag_model_lite)\n",
    "to_json(bag_model_lite,bag_model,'bag')\n",
    "\n",
    "ending_time = time.time()\n",
    "print(ending_time - starting_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3c701d-858f-4eb5-abc2-d3445a5c9ea2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "013f489d-d7b8-4162-bd29-455d02fe5bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_clf = DecisionTreeClassifier(random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7b6dcb75-43f1-4b06-bb5d-2d332ef91a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.846235990524292\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'], # Function used to measure the quality of a split at each node.\n",
    "    'max_depth': list(range(2, 35, 3)) + [None],  # Maximum depth of the tree. None means unlimited depth.\n",
    "    'min_samples_split': list(range(2, 20, 2)),  # Minimum samples required to split an internal node.\n",
    "    'max_features': ['log', 'sqrt']+ [0.1, 0.2, 0.25, 0.33, 0.5],  # Maximum number of features to consider when splitting a node during tree construction. None: can use all available features.\n",
    "}\n",
    "dt_model = {}\n",
    "dt_model['model']='Decision_Tree_Classifier'\n",
    "outer_loop_arr = []\n",
    "dt_model_lite = {}\n",
    "dt_model_lite['model']='Decision_Tree_Classifier'\n",
    "outer_loop_arr_lite = []\n",
    "\n",
    "starting_time = time.time()\n",
    "\n",
    "\n",
    "outer_loop_results = Parallel(n_jobs=-1)(\n",
    "    delayed(process_fold)(dt_clf, param_grid, fold_num, outer_train_index, outer_test_index,False,True)\n",
    "    for fold_num, (outer_train_index, outer_test_index) in enumerate(outer_cv.split(X, y))\n",
    ")\n",
    "\n",
    "# Extract the results for each fold\n",
    "outer_loop_arr = [result[0] for result in outer_loop_results]\n",
    "outer_loop_arr_lite = [result[1] for result in outer_loop_results]\n",
    "\n",
    "# Continue with appending the results to your all_models and all_models_lite lists\n",
    "dt_model['outer_loop'] = outer_loop_arr\n",
    "dt_model_lite['outer_loop'] = outer_loop_arr_lite\n",
    "all_models.append(dt_model)\n",
    "all_models_lite.append(dt_model_lite)\n",
    "\n",
    "to_json(tree_model_lite,tree_model,'tree')\n",
    "\n",
    "ending_time = time.time()\n",
    "print(ending_time - starting_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68971ef",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Gaussian distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c23a210",
   "metadata": {},
   "outputs": [],
   "source": [
    "gausNB_clf = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae9a715b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading bar:\n",
      "\n",
      "10%\n",
      "20%\n",
      "30%\n",
      "40%\n",
      "50%\n",
      "60%\n",
      "70%\n",
      "80%\n",
      "90%\n",
      "100%\n"
     ]
    }
   ],
   "source": [
    "param_grid = {}\n",
    "\n",
    "gausNB_model = {}\n",
    "gausNB_model['model']='Gaussian_distribution_Classifier'\n",
    "outer_loop_arr = []\n",
    "gausNB_model_lite = {}\n",
    "gausNB_model_lite['model']='Gaussian_distribution_Classifier'\n",
    "outer_loop_arr_lite = []\n",
    "print('loading bar:\\n')\n",
    "for fold_num, (outer_train_index, outer_test_index) in enumerate(outer_cv.split(X, y)):\n",
    "    X_outer_train, X_outer_test = Xarr[outer_train_index], Xarr[outer_test_index]\n",
    "    y_outer_train, y_outer_test = yarr[outer_train_index], yarr[outer_test_index]\n",
    "    \n",
    "    best_model,best_param,param_arr,param_arr_lite = grid_search(gausNB_clf, param_grid, X_outer_train, y_outer_train,False,True)\n",
    "    outer_loop,outer_loop_lite=outer_metric(best_model, best_param, X_outer_train, y_outer_train,outer_train_index,\\\n",
    "                 X_outer_test,y_outer_test,outer_test_index,fold_num + 1,False,True)\n",
    "    outer_loop['param_comb']= param_arr\n",
    "    outer_loop_arr.append(outer_loop)\n",
    "    outer_loop_lite['param_comb']= param_arr_lite\n",
    "    outer_loop_arr_lite.append(outer_loop_lite)\n",
    "    print(f'{(fold_num+1)*10}%')\n",
    "gausNB_model['outer_loop']= outer_loop_arr\n",
    "gausNB_model_lite['outer_loop']= outer_loop_arr_lite\n",
    "all_models.append(gausNB_model)\n",
    "all_models_lite.append(gausNB_model_lite)\n",
    "\n",
    "to_json(gausNB_model_lite,gausNB_model,'gausNB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e303a9-987d-4125-97a5-ded87a7fc5d6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95ab30eb-7d27-4df5-8c0a-0d3c63b95872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ccp_alpha': 0.0,\n",
       " 'criterion': 'friedman_mse',\n",
       " 'init': None,\n",
       " 'learning_rate': 0.1,\n",
       " 'loss': 'log_loss',\n",
       " 'max_depth': 3,\n",
       " 'max_features': None,\n",
       " 'max_leaf_nodes': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 100,\n",
       " 'n_iter_no_change': None,\n",
       " 'random_state': 42,\n",
       " 'subsample': 1.0,\n",
       " 'tol': 0.0001,\n",
       " 'validation_fraction': 0.1,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_clf = GradientBoostingClassifier(random_state=42)\n",
    "#gb_clf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "047f60bd-c068-46cd-920e-71494263f495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.822300672531128\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200, 300, 400, 500],  # Number of boosting stages.\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.5, 1.0],  # Shrinkage parameter to control the contribution of each estimator. Small value means each tree in the ensemble has a minor impact on the final prediction lead to  gradual convergence of the algorithm.\n",
    "    'max_depth': list(range(1, 10)),  # Maximum depth of individual decision trees.\n",
    "    'min_samples_split': list(range(2, 21, 2)),  # Minimum samples required to split an internal node.\n",
    "    'subsample': [0.7, 0.8, 0.9, 1.0],  # Fraction of samples used for fitting the trees.\n",
    "    'max_features': ['log', 'sqrt']+ [0.1, 0.2, 0.25, 0.33, 0.5],  # Maximum number of features to consider for a split.\n",
    "}\n",
    "gb_model = {}\n",
    "gb_model['model']='Gradient_Boosting_Classifier'\n",
    "outer_loop_arr = []\n",
    "gb_model_lite = {}\n",
    "gb_model_lite['model']='Gradient_Boosting_Classifier'\n",
    "outer_loop_arr_lite = []\n",
    "\n",
    "starting_time = time.time()\n",
    "\n",
    "\n",
    "outer_loop_results = Parallel(n_jobs=-1)(\n",
    "    delayed(process_fold)(gb_clf, param_grid, fold_num, outer_train_index, outer_test_index,False,True)\n",
    "    for fold_num, (outer_train_index, outer_test_index) in enumerate(outer_cv.split(X, y))\n",
    ")\n",
    "\n",
    "# Extract the results for each fold\n",
    "outer_loop_arr = [result[0] for result in outer_loop_results]\n",
    "outer_loop_arr_lite = [result[1] for result in outer_loop_results]\n",
    "\n",
    "# Continue with appending the results to your all_models and all_models_lite lists\n",
    "gb_model['outer_loop'] = outer_loop_arr\n",
    "gb_model_lite['outer_loop'] = outer_loop_arr_lite\n",
    "all_models.append(gb_model)\n",
    "all_models_lite.append(gb_model_lite)\n",
    "\n",
    "to_json(gb_model_lite,gb_model,'gb')\n",
    "\n",
    "ending_time = time.time()\n",
    "print(ending_time - starting_time)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6ced40",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## KNeighbors Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ea13c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_clf = KNeighborsClassifier(n_jobs = -1)\n",
    "#knn_clf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5419a448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading bar:\n",
      "\n",
      "10%\n",
      "20%\n",
      "30%\n",
      "40%\n",
      "50%\n",
      "60%\n",
      "70%\n",
      "80%\n",
      "90%\n",
      "100%\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_neighbors': list(range(1, 11, 2)),  # Number of neighbors to consider.\n",
    "    'weights': ['uniform', 'distance'],  # Weighting of neighbors. 'uniform': all neighbors have equal weight, 'distance': closer neighbors have more influence.\n",
    "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],  # Algorithm for computing nearest neighbors.\n",
    "    'p': [1, 2],  # Power parameter for Minkowski distance (1 for Manhattan, 2 for Euclidean).\n",
    "}\n",
    "knn_model = {}\n",
    "knn_model['model']='K-Nearest_Neighbors_Classifier'\n",
    "outer_loop_arr = []\n",
    "knn_model_lite = {}\n",
    "knn_model_lite['model']='K-Nearest_Neighbors_Classifier'\n",
    "outer_loop_arr_lite = []\n",
    "print('loading bar:\\n')\n",
    "for fold_num, (outer_train_index, outer_test_index) in enumerate(outer_cv.split(X, y)):\n",
    "    X_outer_train, X_outer_test = Xarr[outer_train_index], Xarr[outer_test_index]\n",
    "    y_outer_train, y_outer_test = yarr[outer_train_index], yarr[outer_test_index]\n",
    "    \n",
    "    best_model,best_param,param_arr,param_arr_lite = grid_search(knn_clf, param_grid, X_outer_train, y_outer_train)\n",
    "    outer_loop,outer_loop_lite=outer_metric(best_model, best_param, X_outer_train, y_outer_train,outer_train_index,\\\n",
    "                 X_outer_test,y_outer_test,outer_test_index,fold_num + 1)\n",
    "    outer_loop['param_comb']= param_arr\n",
    "    outer_loop_arr.append(outer_loop)\n",
    "    outer_loop_lite['param_comb']= param_arr_lite\n",
    "    outer_loop_arr_lite.append(outer_loop_lite)\n",
    "    print(f'{(fold_num+1)*10}%')\n",
    "knn_model['outer_loop']= outer_loop_arr\n",
    "knn_model_lite['outer_loop']= outer_loop_arr_lite\n",
    "all_models.append(knn_model)\n",
    "all_models_lite.append(knn_model_lite)\n",
    "\n",
    "to_json(knn_model_lite,knn_model,'knn')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcdea7d-cca8-4bda-998f-bbf806ab91d1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "233c54e4-1d26-4926-a5bf-1bd1123b7d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_clf = LGBMClassifier(n_jobs = -1, class_weight = 'balanced',random_state = 42,force_col_wise = True)\n",
    "#lgb_clf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e7c4a847-a36b-4c12-8362-36a5c87ca1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading bar:\n",
      "\n",
      "[LightGBM] [Info] Number of positive: 148, number of negative: 2840\n",
      "[LightGBM] [Info] Total Bins 633\n",
      "[LightGBM] [Info] Number of data points in the train set: 2988, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 149, number of negative: 2839\n",
      "[LightGBM] [Info] Total Bins 632\n",
      "[LightGBM] [Info] Number of data points in the train set: 2988, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 149, number of negative: 2839\n",
      "[LightGBM] [Info] Total Bins 632\n",
      "[LightGBM] [Info] Number of data points in the train set: 2988, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 148, number of negative: 2840\n",
      "[LightGBM] [Info] Total Bins 633\n",
      "[LightGBM] [Info] Number of data points in the train set: 2988, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 149, number of negative: 2839\n",
      "[LightGBM] [Info] Total Bins 632\n",
      "[LightGBM] [Info] Number of data points in the train set: 2988, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 149, number of negative: 2839\n",
      "[LightGBM] [Info] Total Bins 632\n",
      "[LightGBM] [Info] Number of data points in the train set: 2988, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 223, number of negative: 4259\n",
      "[LightGBM] [Info] Total Bins 638\n",
      "[LightGBM] [Info] Number of data points in the train set: 4482, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "10%\n",
      "[LightGBM] [Info] Number of positive: 148, number of negative: 2840\n",
      "[LightGBM] [Info] Total Bins 634\n",
      "[LightGBM] [Info] Number of data points in the train set: 2988, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 149, number of negative: 2840\n",
      "[LightGBM] [Info] Total Bins 631\n",
      "[LightGBM] [Info] Number of data points in the train set: 2989, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 149, number of negative: 2840\n",
      "[LightGBM] [Info] Total Bins 631\n",
      "[LightGBM] [Info] Number of data points in the train set: 2989, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 148, number of negative: 2840\n",
      "[LightGBM] [Info] Total Bins 634\n",
      "[LightGBM] [Info] Number of data points in the train set: 2988, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 149, number of negative: 2840\n",
      "[LightGBM] [Info] Total Bins 631\n",
      "[LightGBM] [Info] Number of data points in the train set: 2989, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 149, number of negative: 2840\n",
      "[LightGBM] [Info] Total Bins 631\n",
      "[LightGBM] [Info] Number of data points in the train set: 2989, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 223, number of negative: 4260\n",
      "[LightGBM] [Info] Total Bins 638\n",
      "[LightGBM] [Info] Number of data points in the train set: 4483, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "20%\n",
      "[LightGBM] [Info] Number of positive: 148, number of negative: 2840\n",
      "[LightGBM] [Info] Total Bins 630\n",
      "[LightGBM] [Info] Number of data points in the train set: 2988, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 149, number of negative: 2840\n",
      "[LightGBM] [Info] Total Bins 635\n",
      "[LightGBM] [Info] Number of data points in the train set: 2989, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 149, number of negative: 2840\n",
      "[LightGBM] [Info] Total Bins 634\n",
      "[LightGBM] [Info] Number of data points in the train set: 2989, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 148, number of negative: 2840\n",
      "[LightGBM] [Info] Total Bins 630\n",
      "[LightGBM] [Info] Number of data points in the train set: 2988, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 149, number of negative: 2840\n",
      "[LightGBM] [Info] Total Bins 635\n",
      "[LightGBM] [Info] Number of data points in the train set: 2989, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 149, number of negative: 2840\n",
      "[LightGBM] [Info] Total Bins 634\n",
      "[LightGBM] [Info] Number of data points in the train set: 2989, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 223, number of negative: 4260\n",
      "[LightGBM] [Info] Total Bins 638\n",
      "[LightGBM] [Info] Number of data points in the train set: 4483, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "30%\n",
      "[LightGBM] [Info] Number of positive: 148, number of negative: 2840\n",
      "[LightGBM] [Info] Total Bins 632\n",
      "[LightGBM] [Info] Number of data points in the train set: 2988, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 149, number of negative: 2840\n",
      "[LightGBM] [Info] Total Bins 634\n",
      "[LightGBM] [Info] Number of data points in the train set: 2989, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 149, number of negative: 2840\n",
      "[LightGBM] [Info] Total Bins 634\n",
      "[LightGBM] [Info] Number of data points in the train set: 2989, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 148, number of negative: 2840\n",
      "[LightGBM] [Info] Total Bins 632\n",
      "[LightGBM] [Info] Number of data points in the train set: 2988, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 149, number of negative: 2840\n",
      "[LightGBM] [Info] Total Bins 634\n",
      "[LightGBM] [Info] Number of data points in the train set: 2989, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 149, number of negative: 2840\n",
      "[LightGBM] [Info] Total Bins 634\n",
      "[LightGBM] [Info] Number of data points in the train set: 2989, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 223, number of negative: 4260\n",
      "[LightGBM] [Info] Total Bins 638\n",
      "[LightGBM] [Info] Number of data points in the train set: 4483, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "40%\n",
      "[LightGBM] [Info] Number of positive: 148, number of negative: 2840\n",
      "[LightGBM] [Info] Total Bins 632\n",
      "[LightGBM] [Info] Number of data points in the train set: 2988, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 149, number of negative: 2840\n",
      "[LightGBM] [Info] Total Bins 632\n",
      "[LightGBM] [Info] Number of data points in the train set: 2989, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 149, number of negative: 2840\n",
      "[LightGBM] [Info] Total Bins 634\n",
      "[LightGBM] [Info] Number of data points in the train set: 2989, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 148, number of negative: 2840\n",
      "[LightGBM] [Info] Total Bins 632\n",
      "[LightGBM] [Info] Number of data points in the train set: 2988, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 149, number of negative: 2840\n",
      "[LightGBM] [Info] Total Bins 632\n",
      "[LightGBM] [Info] Number of data points in the train set: 2989, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 149, number of negative: 2840\n",
      "[LightGBM] [Info] Total Bins 634\n",
      "[LightGBM] [Info] Number of data points in the train set: 2989, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 223, number of negative: 4260\n",
      "[LightGBM] [Info] Total Bins 638\n",
      "[LightGBM] [Info] Number of data points in the train set: 4483, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "50%\n",
      "[LightGBM] [Info] Number of positive: 148, number of negative: 2840\n",
      "[LightGBM] [Info] Total Bins 632\n",
      "[LightGBM] [Info] Number of data points in the train set: 2988, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 149, number of negative: 2840\n",
      "[LightGBM] [Info] Total Bins 634\n",
      "[LightGBM] [Info] Number of data points in the train set: 2989, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 149, number of negative: 2840\n",
      "[LightGBM] [Info] Total Bins 633\n",
      "[LightGBM] [Info] Number of data points in the train set: 2989, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 148, number of negative: 2840\n",
      "[LightGBM] [Info] Total Bins 632\n",
      "[LightGBM] [Info] Number of data points in the train set: 2988, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 149, number of negative: 2840\n",
      "[LightGBM] [Info] Total Bins 634\n",
      "[LightGBM] [Info] Number of data points in the train set: 2989, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 149, number of negative: 2840\n",
      "[LightGBM] [Info] Total Bins 633\n",
      "[LightGBM] [Info] Number of data points in the train set: 2989, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 223, number of negative: 4260\n",
      "[LightGBM] [Info] Total Bins 636\n",
      "[LightGBM] [Info] Number of data points in the train set: 4483, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "60%\n",
      "[LightGBM] [Info] Number of positive: 148, number of negative: 2840\n",
      "[LightGBM] [Info] Total Bins 632\n",
      "[LightGBM] [Info] Number of data points in the train set: 2988, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 149, number of negative: 2840\n",
      "[LightGBM] [Info] Total Bins 632\n",
      "[LightGBM] [Info] Number of data points in the train set: 2989, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 149, number of negative: 2840\n",
      "[LightGBM] [Info] Total Bins 634\n",
      "[LightGBM] [Info] Number of data points in the train set: 2989, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 148, number of negative: 2840\n",
      "[LightGBM] [Info] Total Bins 632\n",
      "[LightGBM] [Info] Number of data points in the train set: 2988, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 149, number of negative: 2840\n",
      "[LightGBM] [Info] Total Bins 632\n",
      "[LightGBM] [Info] Number of data points in the train set: 2989, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 149, number of negative: 2840\n",
      "[LightGBM] [Info] Total Bins 634\n",
      "[LightGBM] [Info] Number of data points in the train set: 2989, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 223, number of negative: 4260\n",
      "[LightGBM] [Info] Total Bins 639\n",
      "[LightGBM] [Info] Number of data points in the train set: 4483, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "70%\n",
      "[LightGBM] [Info] Number of positive: 148, number of negative: 2840\n",
      "[LightGBM] [Info] Total Bins 633\n",
      "[LightGBM] [Info] Number of data points in the train set: 2988, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 149, number of negative: 2840\n",
      "[LightGBM] [Info] Total Bins 632\n",
      "[LightGBM] [Info] Number of data points in the train set: 2989, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 149, number of negative: 2840\n",
      "[LightGBM] [Info] Total Bins 633\n",
      "[LightGBM] [Info] Number of data points in the train set: 2989, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 148, number of negative: 2840\n",
      "[LightGBM] [Info] Total Bins 633\n",
      "[LightGBM] [Info] Number of data points in the train set: 2988, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 149, number of negative: 2840\n",
      "[LightGBM] [Info] Total Bins 632\n",
      "[LightGBM] [Info] Number of data points in the train set: 2989, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 149, number of negative: 2840\n",
      "[LightGBM] [Info] Total Bins 633\n",
      "[LightGBM] [Info] Number of data points in the train set: 2989, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 223, number of negative: 4260\n",
      "[LightGBM] [Info] Total Bins 638\n",
      "[LightGBM] [Info] Number of data points in the train set: 4483, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "80%\n",
      "[LightGBM] [Info] Number of positive: 149, number of negative: 2839\n",
      "[LightGBM] [Info] Total Bins 633\n",
      "[LightGBM] [Info] Number of data points in the train set: 2988, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 149, number of negative: 2840\n",
      "[LightGBM] [Info] Total Bins 632\n",
      "[LightGBM] [Info] Number of data points in the train set: 2989, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 150, number of negative: 2839\n",
      "[LightGBM] [Info] Total Bins 632\n",
      "[LightGBM] [Info] Number of data points in the train set: 2989, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 149, number of negative: 2839\n",
      "[LightGBM] [Info] Total Bins 633\n",
      "[LightGBM] [Info] Number of data points in the train set: 2988, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 149, number of negative: 2840\n",
      "[LightGBM] [Info] Total Bins 632\n",
      "[LightGBM] [Info] Number of data points in the train set: 2989, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 150, number of negative: 2839\n",
      "[LightGBM] [Info] Total Bins 632\n",
      "[LightGBM] [Info] Number of data points in the train set: 2989, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 224, number of negative: 4259\n",
      "[LightGBM] [Info] Total Bins 637\n",
      "[LightGBM] [Info] Number of data points in the train set: 4483, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "90%\n",
      "[LightGBM] [Info] Number of positive: 149, number of negative: 2839\n",
      "[LightGBM] [Info] Total Bins 633\n",
      "[LightGBM] [Info] Number of data points in the train set: 2988, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 149, number of negative: 2840\n",
      "[LightGBM] [Info] Total Bins 633\n",
      "[LightGBM] [Info] Number of data points in the train set: 2989, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 150, number of negative: 2839\n",
      "[LightGBM] [Info] Total Bins 634\n",
      "[LightGBM] [Info] Number of data points in the train set: 2989, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 149, number of negative: 2839\n",
      "[LightGBM] [Info] Total Bins 633\n",
      "[LightGBM] [Info] Number of data points in the train set: 2988, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 149, number of negative: 2840\n",
      "[LightGBM] [Info] Total Bins 633\n",
      "[LightGBM] [Info] Number of data points in the train set: 2989, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 150, number of negative: 2839\n",
      "[LightGBM] [Info] Total Bins 634\n",
      "[LightGBM] [Info] Number of data points in the train set: 2989, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 224, number of negative: 4259\n",
      "[LightGBM] [Info] Total Bins 637\n",
      "[LightGBM] [Info] Number of data points in the train set: 4483, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "100%\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200, 300, 400, 500, 700],  # Number of boosting stages. Larger values may lead to better performance but longer training times.\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.5, 1.0],  # Larger values shrinks the contribution of each tree, which can help prevent overfitting but may require more trees for similar predictive power.\n",
    "    'max_depth': list(range(2, 10)),  # Maximum depth of individual trees. Larger values can capture more complex relationships and can lead to overfitting if too large.\n",
    "    'subsample': [0.7, 0.8, 0.9, 1.0],  # Fraction of samples used for fitting trees. A larger value means using more data for training.\n",
    "    'colsample_bytree': [0.7, 0.8, 0.9, 1.0],  # Fraction of features used for fitting trees. A larger value increases diversity but may lead to overfitting if set too high.\n",
    "}\n",
    "lgb_model = {}\n",
    "lgb_model['model']='LightGBM_Classifier'\n",
    "outer_loop_arr = []\n",
    "lgb_model_lite = {}\n",
    "lgb_model_lite['model']='LightGBM_Classifier'\n",
    "outer_loop_arr_lite = []\n",
    "print('loading bar:\\n')\n",
    "for fold_num, (outer_train_index, outer_test_index) in enumerate(outer_cv.split(X, y)):\n",
    "    X_outer_train, X_outer_test = Xarr[outer_train_index], Xarr[outer_test_index]\n",
    "    y_outer_train, y_outer_test = yarr[outer_train_index], yarr[outer_test_index]\n",
    "    \n",
    "    best_model,best_param,param_arr,param_arr_lite = grid_search(lgb_clf, param_grid, X_outer_train, y_outer_train)\n",
    "    outer_loop,outer_loop_lite=outer_metric(best_model, best_param, X_outer_train, y_outer_train,outer_train_index,\\\n",
    "                 X_outer_test,y_outer_test,outer_test_index,fold_num + 1)\n",
    "    outer_loop['param_comb']= param_arr\n",
    "    outer_loop_arr.append(outer_loop)\n",
    "    outer_loop_lite['param_comb']= param_arr_lite\n",
    "    outer_loop_arr_lite.append(outer_loop_lite)\n",
    "    print(f'{(fold_num+1)*10}%')\n",
    "lgb_model['outer_loop']= outer_loop_arr\n",
    "lgb_model_lite['outer_loop']= outer_loop_arr_lite\n",
    "all_models.append(lgb_model)\n",
    "all_models_lite.append(lgb_model_lite)\n",
    "\n",
    "to_json(lgb_model_lite,lgb_model,'lgb')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21031464-0b9c-4cde-bb81-a4eab0db6ed1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Logistic Regression classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0532a7d-e1fa-41ad-8dfe-7efc7ee6d15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_clf = LogisticRegression(class_weight='balanced', n_jobs = -1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb7a7b31-b02a-4213-8102-671665f69186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading bar:\n",
      "\n",
      "10%\n",
      "20%\n",
      "30%\n",
      "40%\n",
      "50%\n",
      "60%\n",
      "70%\n",
      "80%\n",
      "90%\n",
      "100%\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'C': [0.1, 0.5, 1, 2, 10, 100],  # Regularization parameter. Larger values lead to weaker regularization.\n",
    "    'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],  # Kernel function to use.\n",
    "    'degree': [2, 3, 4],  # Degree of the polynomial kernel (used with 'poly' kernel).\n",
    "    'gamma': ['scale', 'auto'] + [0.001, 0.01, 0.1, 1, 10],  # Kernel coefficient for 'rbf', 'poly', and 'sigmoid' kernels. Smaller gamma values lead to smoother decision boundaries witch can overfit the data.\n",
    "    # gamma = scale => gamma = 1/n_features,  gamma = auto => gamma = 1/n_samples.\n",
    "}\n",
    "logreg_model = {}\n",
    "logreg_model['model']='Logistic_Regression_Classifier'\n",
    "outer_loop_arr = []\n",
    "logreg_model_lite = {}\n",
    "logreg_model_lite['model']='Logistic_Regression_Classifier'\n",
    "outer_loop_arr_lite = []\n",
    "print('loading bar:\\n')\n",
    "for fold_num, (outer_train_index, outer_test_index) in enumerate(outer_cv.split(X, y)):\n",
    "    X_outer_train, X_outer_test = Xarr[outer_train_index], Xarr[outer_test_index]\n",
    "    y_outer_train, y_outer_test = yarr[outer_train_index], yarr[outer_test_index]\n",
    "    \n",
    "    best_model,best_param,param_arr,param_arr_lite = grid_search(logreg_clf, param_grid, X_outer_train, y_outer_train)\n",
    "    outer_loop,outer_loop_lite=outer_metric(best_model, best_param, X_outer_train, y_outer_train,outer_train_index,\\\n",
    "                 X_outer_test,y_outer_test,outer_test_index,fold_num + 1)\n",
    "    outer_loop['param_comb']= param_arr\n",
    "    outer_loop_arr.append(outer_loop)\n",
    "    outer_loop_lite['param_comb']= param_arr_lite\n",
    "    outer_loop_arr_lite.append(outer_loop_lite)\n",
    "    print(f'{(fold_num+1)*10}%')\n",
    "logreg_model['outer_loop']= outer_loop_arr\n",
    "logreg_model_lite['outer_loop']= outer_loop_arr_lite\n",
    "all_models.append(logreg_model)\n",
    "all_models_lite.append(logreg_model_lite)\n",
    "\n",
    "to_json(logreg_model_lite,logreg_model,'logreg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff814124-f573-48fd-806d-fbaf0d5ec2f0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "09bf559d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_clf = MLPClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "757114cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.269916772842407\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (150,), (200,), (250,)],  # Number of neurons in each hidden layer. Larger value lead to more complex\n",
    "    'activation': ['identity', 'logistic', 'tanh', 'relu'],  # Activation function for hidden layers. 'identity':  returns its input as-is, 'relu': Rectified Linear Unit\n",
    "    'solver': ['lbfgs', 'sgd', 'adam'],  # Optimization algorithm.\n",
    "    'alpha': np.logspace(-5, 2, 8),  # L2 regularization term. Larger value lead to stronger regularization\n",
    "    'learning_rate': ['constant', 'invscaling', 'adaptive'],  # Learning rate schedule for weight updates.\n",
    "}\n",
    "mlp_model = {}\n",
    "mlp_model['model']='Multi-layer_Perceptron_Classifier'\n",
    "outer_loop_arr = []\n",
    "mlp_model_lite = {}\n",
    "mlp_model_lite['model']='Multi-layer_Perceptron_Classifier'\n",
    "outer_loop_arr_lite = []\n",
    "\n",
    "starting_time = time.time()\n",
    "\n",
    "outer_loop_results = Parallel(n_jobs=-1)(\n",
    "    delayed(process_fold)(mlp_clf, param_grid, fold_num, outer_train_index, outer_test_index)\n",
    "    for fold_num, (outer_train_index, outer_test_index) in enumerate(outer_cv.split(X, y))\n",
    ")\n",
    "\n",
    "# Extract the results for each fold\n",
    "outer_loop_arr = [result[0] for result in outer_loop_results]\n",
    "outer_loop_arr_lite = [result[1] for result in outer_loop_results]\n",
    "\n",
    "# Continue with appending the results to your all_models and all_models_lite lists\n",
    "mlp_model['outer_loop'] = outer_loop_arr\n",
    "mlp_model_lite['outer_loop'] = outer_loop_arr_lite\n",
    "all_models.append(mlp_model)\n",
    "all_models_lite.append(mlp_model_lite)\n",
    "\n",
    "to_json(mlp_model_lite,mlp_model,'mlp')\n",
    "\n",
    "ending_time = time.time()\n",
    "print(ending_time - starting_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3dc881",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## QuadraticDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "22f38b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "qda_clf = QuadraticDiscriminantAnalysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "50834105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading bar:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'priors': [None, [0.1, 0.9], [0.2, 0.8], [0.3, 0.7], [0.4, 0.6]]  # Prior probabilities for each class.\n",
    "}\n",
    "qda_model = {}\n",
    "qda_model['model']='Quadratic_Discriminant_Analysis_Classifier'\n",
    "outer_loop_arr = []\n",
    "qda_model_lite = {}\n",
    "qda_model_lite['model']='Quadratic_Discriminant_Analysis_Classifier'\n",
    "outer_loop_arr_lite = []\n",
    "\n",
    "starting_time = time.time()\n",
    "\n",
    "\n",
    "outer_loop_results = Parallel(n_jobs=-1)(\n",
    "    delayed(process_fold)(qda_clf, param_grid,fold_num, outer_train_index, outer_test_index)\n",
    "    for fold_num, (outer_train_index, outer_test_index) in enumerate(outer_cv.split(X, y))\n",
    ")\n",
    "\n",
    "# Extract the results for each fold\n",
    "outer_loop_arr = [result[0] for result in outer_loop_results]\n",
    "outer_loop_arr_lite = [result[1] for result in outer_loop_results]\n",
    "\n",
    "# Continue with appending the results to your all_models and all_models_lite lists\n",
    "qda_model['outer_loop'] = outer_loop_arr\n",
    "qda_model_lite['outer_loop'] = outer_loop_arr_lite\n",
    "all_models.append(qda_model)\n",
    "all_models_lite.append(qda_model_lite)\n",
    "\n",
    "to_json(qda_model_lite,qda_model,'qda')\n",
    "\n",
    "ending_time = time.time()\n",
    "print(ending_time - starting_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ba365f-4b68-438d-b665-e982b74378d2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## RadiusNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6523fa23-7f51-4e2d-871c-5d277f578ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnc_clf = RadiusNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f1037e73-acb8-432a-99b2-9cabba9b683d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No neighbors found for test samples array([   0,    1,    2, ..., 1491, 1492, 1493], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 463, in _process_worker\n    r = call_item()\n  File \"C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 291, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\joblib\\parallel.py\", line 589, in __call__\n    return [func(*args, **kwargs)\n  File \"C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\joblib\\parallel.py\", line 589, in <listcomp>\n    return [func(*args, **kwargs)\n  File \"C:\\Users\\Dimitar\\AppData\\Local\\Temp\\ipykernel_15068\\3750331053.py\", line 5, in process_fold\n  File \"C:\\Users\\Dimitar\\AppData\\Local\\Temp\\ipykernel_15068\\2087156580.py\", line 77, in grid_search\n  File \"C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\neighbors\\_classification.py\", line 667, in predict\n    probs = self.predict_proba(X)\n  File \"C:\\Users\\Dimitar\\.conda\\envs\\ijs\\lib\\site-packages\\sklearn\\neighbors\\_classification.py\", line 726, in predict_proba\n    raise ValueError(\nValueError: No neighbors found for test samples array([   0,    1,    2, ..., 1491, 1492, 1493], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 18\u001b[0m\n\u001b[0;32m     13\u001b[0m outer_loop_arr_lite \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     15\u001b[0m starting_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 18\u001b[0m outer_loop_results \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_fold\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrnc_clf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfold_num\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mouter_train_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mouter_test_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfold_num\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mouter_train_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mouter_test_index\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mouter_cv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Extract the results for each fold\u001b[39;00m\n\u001b[0;32m     24\u001b[0m outer_loop_arr \u001b[38;5;241m=\u001b[39m [result[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m outer_loop_results]\n",
      "File \u001b[1;32m~\\.conda\\envs\\ijs\\lib\\site-packages\\joblib\\parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1946\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   1947\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   1948\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[0;32m   1949\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   1950\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\ijs\\lib\\site-packages\\joblib\\parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1592\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1594\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1595\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1597\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1598\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1599\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1600\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1601\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\ijs\\lib\\site-packages\\joblib\\parallel.py:1699\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1692\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_retrieval():\n\u001b[0;32m   1693\u001b[0m \n\u001b[0;32m   1694\u001b[0m     \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[0;32m   1695\u001b[0m     \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[0;32m   1696\u001b[0m     \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[0;32m   1697\u001b[0m     \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[0;32m   1698\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aborting:\n\u001b[1;32m-> 1699\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_error_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1700\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1702\u001b[0m     \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1703\u001b[0m     \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\ijs\\lib\\site-packages\\joblib\\parallel.py:1734\u001b[0m, in \u001b[0;36mParallel._raise_error_fast\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1730\u001b[0m \u001b[38;5;66;03m# If this error job exists, immediatly raise the error by\u001b[39;00m\n\u001b[0;32m   1731\u001b[0m \u001b[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[0;32m   1732\u001b[0m \u001b[38;5;66;03m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[0;32m   1733\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1734\u001b[0m     \u001b[43merror_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\ijs\\lib\\site-packages\\joblib\\parallel.py:736\u001b[0m, in \u001b[0;36mBatchCompletionCallBack.get_result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    730\u001b[0m backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel\u001b[38;5;241m.\u001b[39m_backend\n\u001b[0;32m    732\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39msupports_retrieve_callback:\n\u001b[0;32m    733\u001b[0m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[0;32m    734\u001b[0m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[0;32m    735\u001b[0m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[1;32m--> 736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_return_or_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    738\u001b[0m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[0;32m    739\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\.conda\\envs\\ijs\\lib\\site-packages\\joblib\\parallel.py:754\u001b[0m, in \u001b[0;36mBatchCompletionCallBack._return_or_raise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    752\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    753\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m TASK_ERROR:\n\u001b[1;32m--> 754\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[0;32m    755\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[0;32m    756\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: No neighbors found for test samples array([   0,    1,    2, ..., 1491, 1492, 1493], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset."
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'radius': [0.1, 0.5, 1.0, 1.5, 2.0],  # Radius within which neighbors are considered. Smaller radius considers only nearby data points.\n",
    "    'weights': ['uniform', 'distance'],  # Weighting of neighbors. 'uniform': all neighbors have equal weight, 'distance': closer neighbors have more influence.\n",
    "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],  # Algorithm for computing neighbors.\n",
    "    'p': [1, 2],  # Power parameter for Minkowski distance (1 for Manhattan, 2 for Euclidean). Affects distance computation.\n",
    "}\n",
    "rnc_model = {}\n",
    "rnc_model['model']='Radius_Neighbors_Classifier'\n",
    "outer_loop_arr = []\n",
    "rnc_model_lite = {}\n",
    "rnc_model_lite['model']='Radius_Neighbors_Classifier'\n",
    "outer_loop_arr_lite = []\n",
    "\n",
    "starting_time = time.time()\n",
    "\n",
    "\n",
    "outer_loop_results = Parallel(n_jobs=-1)(\n",
    "    delayed(process_fold)(rnc_clf, param_grid,fold_num, outer_train_index, outer_test_index)\n",
    "    for fold_num, (outer_train_index, outer_test_index) in enumerate(outer_cv.split(X, y))\n",
    ")\n",
    "\n",
    "# Extract the results for each fold\n",
    "outer_loop_arr = [result[0] for result in outer_loop_results]\n",
    "outer_loop_arr_lite = [result[1] for result in outer_loop_results]\n",
    "\n",
    "# Continue with appending the results to your all_models and all_models_lite lists\n",
    "rnc_model['outer_loop'] = outer_loop_arr\n",
    "rnc_model_lite['outer_loop'] = outer_loop_arr_lite\n",
    "all_models.append(rnc_model)\n",
    "all_models_lite.append(rnc_model_lite)\n",
    "\n",
    "to_json(rnc_model_lite,rnc_model,'rnc')\n",
    "\n",
    "ending_time = time.time()\n",
    "print(ending_time - starting_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3604cb6d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f09080c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_clf = RandomForestClassifier(class_weight=\"balanced\",random_state=42,n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1672e5c-cd72-40fb-a22d-83d5ae384422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading bar:\n",
      "\n",
      "10%\n",
      "20%\n",
      "30%\n",
      "40%\n",
      "50%\n",
      "60%\n",
      "70%\n",
      "80%\n",
      "90%\n",
      "100%\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200, 300, 400, 500, 1000],  # Number of trees in the forest. More trees usually lead to better performance. Larger values lead to stronger regularization. \n",
    "    'max_depth': list(range(2, 35, 3)) + [None],  # Maximum depth of the trees. None means no maximum depth. Deeper trees can capture more complex patterns but may overfit. Smaller values lead to stronger regularization.\n",
    "    'min_samples_split': list(range(2, 20, 2)),  # Minimum samples required to split an internal node. Larger values help prevent overfitting. Larger values lead to stronger regularization.\n",
    "    'max_features': ['log', 'sqrt']+ [0.1, 0.2, 0.25, 0.33, 0.5],  # Maximum number of features to consider for a split. Smaller values reduce model complexity. Smaller values lead to stronger regularization.\n",
    "    'criterion': ['gini', 'entropy'],  # Criterion for measuring the quality of a split.\n",
    "}\n",
    "forest_model = {}\n",
    "forest_model['model']='Random_Forest_Classifier'\n",
    "outer_loop_arr = []\n",
    "forest_model_lite = {}\n",
    "forest_model_lite['model']='Random_Forest_Classifier'\n",
    "outer_loop_arr_lite = []\n",
    "print('loading bar:\\n')\n",
    "for fold_num, (outer_train_index, outer_test_index) in enumerate(outer_cv.split(X, y)):\n",
    "    X_outer_train, X_outer_test = Xarr[outer_train_index], Xarr[outer_test_index]\n",
    "    y_outer_train, y_outer_test = yarr[outer_train_index], yarr[outer_test_index]\n",
    "    \n",
    "    best_model,best_param,param_arr,param_arr_lite = grid_search(forest_clf, param_grid, X_outer_train, y_outer_train)\n",
    "    outer_loop,outer_loop_lite=outer_metric(best_model, best_param, X_outer_train, y_outer_train,outer_train_index,\\\n",
    "                 X_outer_test,y_outer_test,outer_test_index,fold_num + 1)\n",
    "    outer_loop['param_comb']= param_arr\n",
    "    outer_loop_arr.append(outer_loop)\n",
    "    outer_loop_lite['param_comb']= param_arr_lite\n",
    "    outer_loop_arr_lite.append(outer_loop_lite)\n",
    "    print(f'{(fold_num+1)*10}%')\n",
    "forest_model['outer_loop']= outer_loop_arr\n",
    "forest_model_lite['outer_loop']= outer_loop_arr_lite\n",
    "all_models.append(forest_model)\n",
    "all_models_lite.append(forest_model_lite)\n",
    "\n",
    "to_json(forest_model_lite,forest_model,'rf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6333fab2-04e3-49fe-8298-2bb26c9ec0c6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Ridge Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57afae75-160c-4ec1-ad77-e8eaf88d27c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 1.0,\n",
       " 'class_weight': None,\n",
       " 'copy_X': True,\n",
       " 'fit_intercept': True,\n",
       " 'max_iter': None,\n",
       " 'positive': False,\n",
       " 'random_state': None,\n",
       " 'solver': 'auto',\n",
       " 'tol': 0.0001}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_clf = RidgeClassifier(class_weight = 'balanced', random_state = 42)\n",
    "#ridge_clf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35d77e1-cacf-47b6-bdfd-bf87575fac5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'alpha': np.logspace(-5, 2, 8),  # Regularization strength (L2 regularization). Smaller values lead to weaker regularization.\n",
    "    'solver' : ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga', 'lbfgs'],  # Algorithm for optimization.\n",
    "    'max_iter': [None, 50, 100, 200, 300, 400, 500, 1000],  # Maximum number of optimization iterations. If None the model takes the default for each solver.\n",
    "}\n",
    "ridge_model = {}\n",
    "ridge_model['model']='Ridge_Classifier'\n",
    "outer_loop_arr = []\n",
    "ridge_model_lite = {}\n",
    "ridge_model_lite['model']='Ridge_Classifier'\n",
    "outer_loop_arr_lite = []\n",
    "\n",
    "starting_time = time.time()\n",
    "\n",
    "outer_loop_results = Parallel(n_jobs=-1)(\n",
    "    delayed(process_fold)(ridge_reg, param_grid, fold_num, outer_train_index, outer_test_index)\n",
    "    for fold_num, (outer_train_index, outer_test_index) in enumerate(outer_cv.split(X, y))\n",
    ")\n",
    "\n",
    "# Extract the results for each fold\n",
    "outer_loop_arr = [result[0] for result in outer_loop_results]\n",
    "outer_loop_arr_lite = [result[1] for result in outer_loop_results]\n",
    "\n",
    "# Continue with appending the results to your all_models and all_models_lite lists\n",
    "ridge_model['outer_loop'] = outer_loop_arr\n",
    "ridge_model_lite['outer_loop'] = outer_loop_arr_lite\n",
    "all_models.append(ridge_model)\n",
    "all_models_lite.append(ridge_model_lite)\n",
    "\n",
    "to_json(ridge_model_lite,ridge_model,'ridge')\n",
    "\n",
    "ending_time = time.time()\n",
    "print(ending_time - starting_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b79d76-7546-4f23-acad-a679f7c1fc3b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2daa92f1-caf8-4229-8b9f-7520a5567414",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_clf = SGDClassifier(n_jobs = -1, class_weight = 'balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a965a981-c30a-40a8-a7ee-d143b88c1595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading bar:\n",
      "\n",
      "10%\n",
      "20%\n",
      "30%\n",
      "40%\n",
      "50%\n",
      "60%\n",
      "70%\n",
      "80%\n",
      "90%\n",
      "100%\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'loss': ['hinge', 'log', 'modified_huber'],  # Loss function to use for optimization.\n",
    "    'penalty': ['l2', 'l1', 'elasticnet'],  # Penalty term for regularization.\n",
    "    'alpha': np.logspace(-5, 2, 8),  # Regularization strength. Larger values lead to stronger regularization. \n",
    "    'max_iter': [50, 100, 200, 300, 400, 500, 1000],  # Maximum number of iterations.\n",
    "}\n",
    "sgd_model = {}\n",
    "sgd_model['model']='SGD_Classifier'\n",
    "outer_loop_arr = []\n",
    "sgd_model_lite = {}\n",
    "sgd_model_lite['model']='SGD_Classifier'\n",
    "outer_loop_arr_lite = []\n",
    "print('loading bar:\\n')\n",
    "for fold_num, (outer_train_index, outer_test_index) in enumerate(outer_cv.split(X, y)):\n",
    "    X_outer_train, X_outer_test = Xarr[outer_train_index], Xarr[outer_test_index]\n",
    "    y_outer_train, y_outer_test = yarr[outer_train_index], yarr[outer_test_index]\n",
    "    \n",
    "    best_model,best_param,param_arr,param_arr_lite = grid_search(sgd_clf, param_grid, X_outer_train, y_outer_train, True, False)\n",
    "    outer_loop,outer_loop_lite=outer_metric(best_model, best_param, X_outer_train, y_outer_train,outer_train_index,\\\n",
    "                 X_outer_test,y_outer_test,outer_test_index,fold_num + 1,True, False)\n",
    "    outer_loop['param_comb']= param_arr\n",
    "    outer_loop_arr.append(outer_loop)\n",
    "    outer_loop_lite['param_comb']= param_arr_lite\n",
    "    outer_loop_arr_lite.append(outer_loop_lite)\n",
    "    print(f'{(fold_num+1)*10}%')\n",
    "sgd_model['outer_loop']= outer_loop_arr\n",
    "sgd_model_lite['outer_loop']= outer_loop_arr_lite\n",
    "all_models.append(sgd_model)\n",
    "all_models_lite.append(sgd_model_lite)\n",
    "\n",
    "to_json(sgd_model_lite,sgd_model,'sgd')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc416cd3-f3de-4717-87f8-4203a0fafb51",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Support Vector Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae5909ae-fb69-4712-991c-ce41b39e116b",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_clf = SVC(class_weight='balanced', probability=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d51ea7b-4a91-4a14-9624-a8aaf4eb9ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59.21150255203247\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'C': [0.1, 0.5, 1, 2, 10, 100],  # Regularization parameter. Larger values allow for more flexible decision boundaries but may overfit.\n",
    "    'kernel': ['linear', 'rbf', 'poly' , 'sigmoid'],  # Kernel function for mapping data to a higher-dimensional space. Functions: Linear, Radial basis function (RBF), Polynomial.\n",
    "    'degree': [2, 3, 4],  # Degree of the polynomial kernel (used with 'poly' kernel).\n",
    "    'gamma': ['scale', 'auto'] + [0.001, 0.01, 0.1, 1, 10],  # Kernel coefficient for 'rbf', 'poly', and 'sigmoid' kernels. Smaller gamma values lead to smoother decision boundaries witch can overfit the data.\n",
    "    # gamma = scale => gamma = 1/n_features,  gamma = auto => gamma = 1/n_samples.\n",
    "}\n",
    "svc_model = {}\n",
    "svc_model['model']='Support_Vector_Classifier'\n",
    "outer_loop_arr = []\n",
    "svc_model_lite = {}\n",
    "svc_model_lite['model']='Support_Vector_Classifier'\n",
    "outer_loop_arr_lite = []\n",
    "\n",
    "starting_time = time.time()\n",
    "\n",
    "\n",
    "outer_loop_results = Parallel(n_jobs=-1)(\n",
    "    delayed(process_fold)(svc_clf, param_grid, fold_num, outer_train_index, outer_test_index)\n",
    "    for fold_num, (outer_train_index, outer_test_index) in enumerate(outer_cv.split(X, y))\n",
    ")\n",
    "\n",
    "# Extract the results for each fold\n",
    "outer_loop_arr = [result[0] for result in outer_loop_results]\n",
    "outer_loop_arr_lite = [result[1] for result in outer_loop_results]\n",
    "\n",
    "# Continue with appending the results to your all_models and all_models_lite lists\n",
    "svc_model['outer_loop'] = outer_loop_arr\n",
    "svc_model_lite['outer_loop'] = outer_loop_arr_lite\n",
    "all_models.append(svc_model)\n",
    "all_models_lite.append(svc_model_lite)\n",
    "\n",
    "to_json(svc_model_lite,svc_model,'svc')\n",
    "\n",
    "ending_time = time.time()\n",
    "print(ending_time - starting_time)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ebb33e-2a80-415e-9bfc-1c0e5a685da4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfae105e",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_clf = XGBClassifier(random_state=42, n_jobs = -1)\n",
    "#xgb_clf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6a3af1a1-1d83-44a6-9d80-0447006ab352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading bar:\n",
      "\n",
      "10%\n",
      "20%\n",
      "30%\n",
      "40%\n",
      "50%\n",
      "60%\n",
      "70%\n",
      "80%\n",
      "90%\n",
      "100%\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200, 300, 400, 500, 700],  # Number of boosting stages. Larger values may lead to better performance but longer training times.\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.5, 1.0], # Larger values shrinks the contribution of each tree, which can help prevent overfitting but may require more trees for similar predictive power.\n",
    "    'max_depth': list(range(1, 10)),  # Maximum depth of individual trees. Larger values can capture more complex relationships and can lead to overfitting if too large.\n",
    "    'subsample': [0.7, 0.8, 0.9, 1.0],  # Fraction of samples used for fitting trees.  Smaller values reduce overfitting risk. \n",
    "    'colsample_bytree': [0.7, 0.8, 0.9, 1.0],  # Fraction of features used for fitting trees. A larger value increases diversity but may lead to overfitting if set too high.\n",
    "    'objective': ['binary:logistic'],  # Learning task and objective function for binary classification.\n",
    "    'eval_metric': ['logloss', 'auc'],  # Evaluation metric to optimize. Logloss measures classification accuracy, AUC measures area under the ROC curve.\n",
    "}\n",
    "xgb_model = {}\n",
    "xgb_model['model']='XGB_Classifier'\n",
    "outer_loop_arr = []\n",
    "xgb_model_lite = {}\n",
    "xgb_model_lite['model']='XGB_Classifier'\n",
    "outer_loop_arr_lite = []\n",
    "print('loading bar:\\n')\n",
    "for fold_num, (outer_train_index, outer_test_index) in enumerate(outer_cv.split(X, y)):\n",
    "    X_outer_train, X_outer_test = Xarr[outer_train_index], Xarr[outer_test_index]\n",
    "    y_outer_train, y_outer_test = yarr[outer_train_index], yarr[outer_test_index]\n",
    "    \n",
    "    best_model,best_param,param_arr,param_arr_lite = grid_search(xgb_clf, param_grid, X_outer_train, y_outer_train,False,True)\n",
    "    outer_loop,outer_loop_lite=outer_metric(best_model, best_param, X_outer_train, y_outer_train,outer_train_index,\\\n",
    "                 X_outer_test,y_outer_test,outer_test_index,fold_num + 1,False,True)\n",
    "    outer_loop['param_comb']= param_arr\n",
    "    outer_loop_arr.append(outer_loop)\n",
    "    outer_loop_lite['param_comb']= param_arr_lite\n",
    "    outer_loop_arr_lite.append(outer_loop_lite)\n",
    "    print(f'{(fold_num+1)*10}%')\n",
    "xgb_model['outer_loop']= outer_loop_arr\n",
    "xgb_model_lite['outer_loop']= outer_loop_arr_lite\n",
    "all_models.append(xgb_model)\n",
    "all_models_lite.append(xgb_model_lite)\n",
    "\n",
    "to_json(xgb_model_lite,xgb_model,'xgb')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33aee99-d6ac-4b00-90dd-4ef314b0dddb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## making the JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4329f425-590c-4b20-9aec-59cb6e434c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Convert the array of dictionaries to a JSON array using the custom encoder\n",
    "#json_array = json.dumps(all_models, cls=NumpyArrayEncoder,indent=2)\n",
    "\n",
    "# Save the JSON array to a file\n",
    "#with open(f'result_dataset{momental_dataset}/allmodels_lite.json', 'w') as json_file:\n",
    "    #json_file.write(json_array)\n",
    "\n",
    "\n",
    "json_array = json.dumps(all_models_lite, cls=NumpyArrayEncoder,indent=2)\n",
    "\n",
    "# Save the JSON array to a file\n",
    "with open(f'result_dataset{momental_dataset}/allmodels_lite.json', 'w') as json_file:\n",
    "    json_file.write(json_array)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
